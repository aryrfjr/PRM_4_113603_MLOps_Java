services:

  # TODO: Avoid production impact by creating a docker-compose.dev.ym

  # PostgreSQL database service
  # Uses a public official image (no build needed)
  postgres:
    image: postgres:16
    restart: always
    environment:
      POSTGRES_DB: mlops
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
    ports:
      - "5433:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./db:/docker-entrypoint-initdb.d
    healthcheck:
      test: [ "CMD", "pg_isready", "-U", "postgres" ]
      interval: 10s
      retries: 5
    networks:
      - mlopsnet

  # MLOps Gateway REST API with Spring Framework
  mlops-api:
    build:
      context: ./mlops-api
      args:
        GITHUB_USERNAME: aryrfjr
        GITHUB_TOKEN: ${GITHUB_TOKEN}
    depends_on:
      - postgres
    ports:
      - "8080:8080"
    environment:
      SPRING_DATASOURCE_URL: jdbc:postgresql://postgres:5432/mlops
      SPRING_DATASOURCE_USERNAME: postgres
      SPRING_DATASOURCE_PASSWORD: postgres
    networks:
      - mlopsnet
    volumes:
      - /home/aryjr/fromiomega/pos-doc/UFSCar/MG-NMR:/data
      - /home/aryjr/.m2/settings.xml:/root/.m2/settings.xml

  # MLOps Microservices REST API
  mlops-ms-api:
    build: ./mlops-ms-api
    ports:
      - "8000:8000"
    volumes:
      - ./mlops-ms-api:/app  # <--- only for DEV, remove for PROD
      - /home/aryjr/fromiomega/pos-doc/UFSCar/MG-NMR:/data  # raw data accessible to the API
    networks:
      - mlopsnet
    environment:
      - PYTHONUNBUFFERED=1 # logs and print() statements will flush instantly ... more reliable debugging
    command: >
      uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload

  # Simulated HPC service
  mlops-hpc:
    build:
      context: ./hpc-mock
      args:
        GITHUB_USERNAME: aryrfjr
        GITHUB_TOKEN: ${GITHUB_TOKEN}
    depends_on:
      - minio
    ports:
      - "8082:8080"
    networks:
      - mlopsnet
    volumes:
      - /home/aryjr/fromiomega/pos-doc/UFSCar/MG-NMR:/data
      - /home/aryjr/.m2/settings.xml:/root/.m2/settings.xml

  # MinIO Object Storage
  minio:
    image: minio/minio
    container_name: minio
    restart: always
    ports:
      - "9000:9000"  # S3 API
      - "9001:9001"  # Web UI
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server /data --console-address ":9001"
    volumes:
      - minio_data:/data
    networks:
      - mlopsnet

  # Airflow Webserver UI
  # Uses the official Airflow image (no build needed)
  airflow-webserver:
    image: apache/airflow:2.8.1
    restart: always
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CORE__FERNET_KEY: ''
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'false'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: 'true'
      AIRFLOW__API__AUTH_BACKENDS: airflow.api.auth.backend.basic_auth
      HPC_API_URL: http://mlops-hpc:8080  # Internal Docker network communication with the Simulated HPC service
      MS_API_URL: http://mlops-ms-api:8000  # Internal Docker network communication with the MLOps Microservices API
    ports:
      - "8084:8080"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/requirements.txt:/requirements.txt
    command: bash -c "pip install --no-cache-dir -r /requirements.txt && airflow webserver"
    networks:
      - mlopsnet

  # Airflow Scheduler
  # Uses the official Airflow image (no build needed)
  airflow-scheduler:
    image: apache/airflow:2.8.1
    restart: always
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CORE__FERNET_KEY: ''
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'false'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__API__AUTH_BACKENDS: airflow.api.auth.backend.basic_auth
      HPC_API_URL: http://mlops-hpc:8080  # Internal Docker network communication with the Simulated HPC service
      MS_API_URL: http://mlops-ms-api:8000  # Internal Docker network communication with the MLOps Microservices API
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/requirements.txt:/requirements.txt
    command: bash -c "pip install --no-cache-dir -r /requirements.txt && airflow scheduler"
    networks:
      - mlopsnet

  # Airflow Init - runs DB migrations and creates the admin user
  # Uses the official Airflow image (no build needed)
  airflow-init:
    image: apache/airflow:2.8.1
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
    entrypoint: bash
    command: -c "
      airflow db migrate && \
      airflow users create \
      --username admin \
      --password admin \
      --firstname Admin \
      --lastname User \
      --role Admin \
      --email admin@example.com"
    networks:
      - mlopsnet

  # Kafka broker service. Used by Airflow to send events and by Spring Boot (mlops-api) to consume them
  kafka:
    image: bitnami/kafka:latest
    restart: always
    ports:
      - "9092:9092"
    environment:
      - KAFKA_KRAFT_MODE=true
      - KAFKA_CFG_PROCESS_ROLES=broker,controller
      - KAFKA_CFG_NODE_ID=1
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=1@kafka:9093
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=true
      - KAFKA_CFG_LOG_DIRS=/bitnami/kafka/data
    volumes:
      - kafka_data:/bitnami/kafka
    networks:
      - mlopsnet

# Docker networks
networks:
  mlopsnet:
    driver: bridge

# Docker-managed persistent volume for PostgreSQL data
volumes:
  postgres_data:
  minio_data:
  kafka_data:
